{"total_rows":159,"bookmark":"g1AAAADaeJzLYWBgYM5gTmGQT0lKzi9KdUhJMtZLykxPyilN1UvOyS9NScwr0ctLLckBKmRKZEiy____fxaY4-ZYbD5nA5CVKJ2FaoIRbhPyWIAkQwOQApqzH2HQQgOQQSqkGnQAYhCSixY-ABmkn5UFAJFDQzo","rows":[{"id":"60a491d4e7c167ad8a38ac6595dd0e56bd5b44f6","order":[20151001.0,37],"fields":{},"doc":{"_id":"60a491d4e7c167ad8a38ac6595dd0e56bd5b44f6","_rev":"2-ef8bd8c70979bafa17ca7690d7d0d0a4","created_at":"2015-10-01 19:49:08 +00:00","full_name":"Cloud Data Services","url":"https://developer.ibm.com/clouddataservices/docs/dataworks/","status":"Live","name":"Introduction to DataWorks","description":"IBM DataWorks mixes and refines raw data from both on-premises and off-premises environments. ","languages":[],"technologies":["DataWorks"],"topic":["Migration"],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://youtu.be/C4XnZSkCDxs","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 19:49:08 +00:00"}},{"id":"fc60323477506db4b0f6424eb4b73202eea6903d","order":[20151001.0,37],"fields":{},"doc":{"_id":"fc60323477506db4b0f6424eb4b73202eea6903d","_rev":"2-42040dc17fa73d97ac4cbe813df55d5d","created_at":"2015-10-01 21:13:27 +00:00","full_name":"Get started with dashDB on Bluemix","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/get/get-started-with-dashdb-on-bluemix/","status":"Live","name":"Watch how to get started with dashDB on Bluemix","description":"See how quick and easy it is to set up a dashDB instance in IBM Bluemix and load data to perform analytics in dashDB.","languages":[],"technologies":["Bluemix","dashDB"],"topic":["Analytics","Data Warehousing"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=-ar3xfg0uGw","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:13:27 +00:00"}},{"id":"01030c4838ae298ea2f671eda79e2365feffff63","order":[20151001.0,38],"fields":{},"doc":{"_id":"01030c4838ae298ea2f671eda79e2365feffff63","_rev":"2-35e65c36fabf460b7ef4114d28222b37","created_at":"2015-10-01 20:42:32 +00:00","full_name":"Cloud Data Services","url":"https://developer.ibm.com/clouddataservices/docs/dataworks/dataworks-forge/","status":"Live","name":"IBM DataWorks Forge","description":"Load data from data sources like SQL Database and dashDB, understand it's quality, refine the data, and then deliver the refined data to systems and applications.","languages":[],"technologies":["DataWorks"],"topic":["Migration"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"http://ibmtvdemo.edgesuite.net/software/dataworks/getting-started/getting-started-with-dataworks-forge.html","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 20:42:32 +00:00"}},{"id":"e1ea18968e3b8692f607cc9c56f8d383165799e7","order":[20151001.0,38],"fields":{},"doc":{"_id":"e1ea18968e3b8692f607cc9c56f8d383165799e7","_rev":"2-5b87ec7f23904b0fd2de5146790fe022","created_at":"2015-10-01 21:22:06 +00:00","full_name":"Extract and export dashDB data to a CSV file","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/analyze/extract-and-export-dashdb-data-to-a-csv-file/","status":"Live","name":"Extract and export dashDB data to a CSV file","description":"Watch how to extract and export dashDB data to a CSV file for import into a spreadsheet application such as Microsoft Excel or Open Office. ","languages":[],"technologies":["dashDB"],"topic":["Analytics","Migration"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=KD3cbTfmzUc","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:22:06 +00:00"}},{"id":"2f362e3286ac1d4ebe1dd58af02b4de14bf0bd6d","order":[20151001.0,39],"fields":{},"doc":{"_id":"2f362e3286ac1d4ebe1dd58af02b4de14bf0bd6d","_rev":"2-4f594fcfe2ff19a74c2117cdd3c79810","created_at":"2015-10-01 21:16:39 +00:00","full_name":"Load XML data into dashDB","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/get/load-xml-data-into-dashdb/","status":"Live","name":"Load XML data into dashDB","description":"Watch how to convert XML data to CSV format to load into dashDB. This video shows a tool called Convert XML to CSV found here: http://www.convertcsv.com/xml-to-csv.htm","languages":[],"technologies":["dashDB"],"topic":["Migration"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=xQqsKz7shUI","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:16:39 +00:00"}},{"id":"d456fef4a7e79eae124d585eaad414d594e48d78","order":[20151001.0,40],"fields":{},"doc":{"_id":"d456fef4a7e79eae124d585eaad414d594e48d78","_rev":"2-1d417de237cdb56457e06d212cfb4bb4","created_at":"2015-10-01 21:18:12 +00:00","full_name":"Perform market basket analysis using dashDB and R","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/analyze/perform-market-basket-analysis-using-dashdb-and-r/","status":"Live","name":"Perform market basket analysis using dashDB and R","description":"Watch how to apply association rules using R to data stored in dashDB using a retail market basket scenario. ","languages":["R"],"technologies":["dashDB"],"topic":["Analytics"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=U4cATqOvmH0","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:18:12 +00:00"}},{"id":"a6bd327f50b77583443fb9ece9890d735081600f","order":[20151001.0,52],"fields":{},"doc":{"_id":"a6bd327f50b77583443fb9ece9890d735081600f","_rev":"2-de9f172e95583f1c30e1daeb82634030","created_at":"2015-10-01 21:15:04 +00:00","full_name":"Store Tweets Using Bluemix, Node-RED, Cloudant, and dashDB","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/get/store-tweets-using-bluemix-node-red-cloudant-and-dashdb/","status":"Live","name":"Store Tweets Using Bluemix, Node-RED, Cloudant, and dashDB","description":"Watch how to create a Bluemix application using Node-RED that searches for tweets and stores the resulting tweets in a Cloudant NoSQL database, then use the data warehousing tool built into Cloudant to load the data into a dashDB.","languages":[],"technologies":["Bluemix","Cloudant","dashDB"],"topic":["Data Warehousing"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=QSnzz3pKDNE","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:15:04 +00:00"}},{"id":"cae9e7234d5cbd38e421df0923bb545e8c6b0dd7","order":[20151001.0,53],"fields":{},"doc":{"_id":"cae9e7234d5cbd38e421df0923bb545e8c6b0dd7","_rev":"2-febfc3bcd94a9ea3ec1b1344b514a148","created_at":"2015-10-01 21:19:32 +00:00","full_name":"Leverage dashDB in Cognos Business Intelligence","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/analyze/leverage-dashdb-in-cognos-business-intelligence/","status":"Live","name":"Leverage dashDB in Cognos Business Intelligence","description":"Learn how to configure a dashDB connection in Cognos Business Intelligence and create stunning visualizations. ","languages":[],"technologies":["dashDB"],"topic":["Analytics"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=EAr8sWEtx6A","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:19:32 +00:00"}},{"id":"4413de0a2ef4c74a6d301b97d4e4bfadaf5e0ec9","order":[20151001.0,54],"fields":{},"doc":{"_id":"4413de0a2ef4c74a6d301b97d4e4bfadaf5e0ec9","_rev":"2-e3df19977f506f8a5d711af20feca75e","created_at":"2015-10-01 21:20:52 +00:00","full_name":"Integrate dashDB with Excel","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/analyze/integrate-dashdb-with-excel/","status":"Live","name":"Integrate dashDB with Excel","description":"Love to work in Microsoft Excel? Watch how to connect to IBM dashDB as the data source for Excel, and how to import tables into a spreadsheet. ","languages":[],"technologies":["dashDB"],"topic":["Analytics"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=oRuN9bniAxU","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:20:52 +00:00"}},{"id":"bbd45ab50483e6d7d1c0ed4ee0f629d15a111162","order":[20150922.0,36],"fields":{},"doc":{"_id":"bbd45ab50483e6d7d1c0ed4ee0f629d15a111162","_rev":"3-e33d92bf3ebc3a2a9aed782e3ab7b675","created_at":"2015-09-22 18:28:19 +00:00","full_name":"Export Cloudant JSON as CSV, RSS, or iCal","url":"https://developer.ibm.com/clouddataservices/2015/09/22/export-cloudant-json-as-csv-rss-or-ical/","status":"Live","name":"Export Cloudant JSON as CSV, RSS, or iCal","description":"Need to convert your Cloudant JSON to a different format? Read how to get JSON data into a spreadsheet, into your calendar, or aggregate data in an RSS reader.","languages":[],"technologies":["Cloudant"],"topic":[],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-09-22 18:28:48 +00:00"}},{"id":"fc97e099d61423e3fe13bd977e2149ad85f44a78","order":[20150824.0,48],"fields":{},"doc":{"_id":"fc97e099d61423e3fe13bd977e2149ad85f44a78","_rev":"3-55a98b5c86a763af7487f84e87c7ff66","created_at":"2015-08-24 17:21:10 +00:00","full_name":"Your First Data Warehouse Is Easy. Meet the ODS.","url":"https://developer.ibm.com/clouddataservices/2015/08/24/operational-data-store/","status":"Live","name":"Your First Data Warehouse Is Easy. Meet the ODS.","description":"Building your first data warehouse doesn’t have to be as enterprise-y and scary as it sounds, especially in the era of cloud services. A good first iteration of your warehousing environment is a simple architecture that we BI architects like to call an “Operational Data Store” (ODS).","languages":[],"technologies":["dashDB"],"topic":["Data Warehousing"],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-24 17:21:30 +00:00"}},{"id":"36f6d18d91505d767abd3f93754dda722164e38c","order":[20150824.0,49],"fields":{},"doc":{"_id":"36f6d18d91505d767abd3f93754dda722164e38c","_rev":"2-628c7e74a22631266f0f2ab5816d313b","created_at":"2015-08-24 17:25:21 +00:00","full_name":"Speed your SQL Queries with Spark SQL","url":"https://developer.ibm.com/clouddataservices/2015/08/19/speed-your-sql-queries-with-spark-sql/","status":"Live","name":"Speed your SQL Queries with Spark SQL","description":"Get faster queries and write less code too. Learn how to use Spark SQL to query your relational database. Follow this tutorial and see how to query a cloud-based Compose PostgreSQL instance or a local PostreSQL database.","languages":["SQL"],"technologies":["Spark","PostgreSQL"],"topic":["SQL"],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-24 17:25:21 +00:00"}},{"id":"425e13adcde45fef1910b32895137ba3cae4be5b","order":[20150824.0,50],"fields":{},"doc":{"_id":"425e13adcde45fef1910b32895137ba3cae4be5b","_rev":"3-e64f87e5baada158a04d44df3253a008","created_at":"2015-08-24 17:58:20 +00:00","full_name":"How to analyze your pipe runs with Bunyan","url":"https://developer.ibm.com/clouddataservices/2015/08/11/analyze-pipe-runs-bunyan/","status":"Live","name":"How to analyze your pipe runs with Bunyan","description":"How to use Bunyan to capture detailed logging of data migration runs through our Simple Data Pipes app.","languages":[],"technologies":["Cloudant"],"topic":["Migration","NoSQL"],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-24 17:58:41 +00:00"}},{"id":"476f784bbdab5efe54a2ed5e74f39504e7531395","order":[20150814.0,47],"fields":{},"doc":{"_id":"476f784bbdab5efe54a2ed5e74f39504e7531395","_rev":"3-f1b1edc5204cead6d7ac4fbae96225d3","created_at":"2015-08-14 09:57:26 +00:00","full_name":"Writing Data Directly to Cloudant from Slack","url":"https://developer.ibm.com/clouddataservices/2015/08/13/writing-data-directly-cloudant-slack/","status":"Live","name":"Writing Data Directly to Cloudant from Slack","description":"Slack's integration API allows external services to be plugged in with ease. Even if your service isn't listed in the off-the-shelf integrations, you can still push data to other HTTP services. This tutorial shows how a Slack 'slash command' can be configured to push data to a Cloudant or CouchDB database in a few easy steps.","languages":["JavaScript"],"technologies":["Cloudant"],"topic":["NoSQL"],"type":"Tutorial","level":"Beginner","imageurl":"http://developer.ibm.com/clouddataservices/wp-content/uploads/sites/47/2015/08/sl_2.png","body":"Slack’s Integration API and Cloudant’s HTTP API make it simple to store data directly into a Cloudant database without breaking a sweat. This tutorial shows how to create a custom slash command in Slack and how to post it directly to Cloudant.\r\n\r\nSlack is a messaging and team-working application that is used widely to allow disparate teams of people to chat, share files, and interact on desktop, tablet, and mobile platforms. We use Slack in IBM Cloud Data Services to coordinate our activities, to work in an open collaborative environment, and to cut down on email and meetings.\r\n\r\nOne of the strengths of Slack is that it integrates with other web services, so events happening in Github or Stack Overflow can be surfaced in the appropriate Slack channels. Slack also has an API that lets you create custom integrations. The simplest of these is slash commands: when a user starts a Slack message with a forward slash followed by a command string, Slack can be configured to POST that data to an external API. Say you create the slash command /lunch. A user could type:","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-14 09:57:35 +00:00"}},{"id":"c716564ddbd0e085ab3ee9af11552fc209a60994","order":[20150804.0,35],"fields":{},"doc":{"_id":"c716564ddbd0e085ab3ee9af11552fc209a60994","_rev":"3-22e53c15e79692a2bb882c65615e404e","created_at":"2015-08-04 15:26:08 +00:00","full_name":"Getting Started with Elasticsearch using Compose","url":"https://www.compose.io/articles/getting-started-with-elasticsearch-using-compose/","status":"Live","name":"Getting Started with Elasticsearch using Compose","description":"How to set up an Elasticsearch database on Compose.","languages":["HTTP","JavaScript"],"technologies":["MongoDB"],"topic":[],"type":"Article","level":"Beginner","imageurl":"","body":"Hi again! In this latest part of our tour through getting started with the multiple, or singular, database joys of Compose we'll be looking at setting up Elasticsearch. In the first part, we covered MongoDB on Compose and the sign up process is basically the same for both. If you didn't read that article, have a browse now...\r\n\r\nThere is of course one difference, obviously you'll need to select Elasticsearch in the Choose a Database panel and take a note that the Elasticsearch pricing is different to MongoDB's. If you're on the thirty day trial, that won't bother you immediately, but do take a note.\r\n\r\nAnyway, back to the Elasticsearch database setup. Once you have signed in, you'll be greeted by the Jobs screen showing that Compose has created your Elasticsearch database.\r\n\r\nYou're next stop should be to select the Overview option in the  sidebar. This will show you various, useful, items of information at the top, but, right now, it'll be showing a message at the top of the page...\r\n\r\nYes, but what you've got is an account which you can log in to with your email address and password for the Compose Dashboard and your own Compose account. Those credentials are only ever used to log into the Compose Dashboard. As we explain in the MongoDB walkthrough, each database has it's own credential system to be managed. For Elasticsearch those credentials are usernames and passwords for the HTTP/TCP access portal which protects your Elasticsearch cluster.\r\n\r\nThere's actually two HTTP/TCP access portals to enable redundancy in accessing the cluster and both get automatically updated with the same credentials. All you need to do is create your user/password pair and you can  connect to either of the portals (or both if your driver supports multiple URLs to connect with). Click on Users in the left hand sidebar and then click Add user.\r\n\r\nYou'll be prompted to enter a username and password here; make them different from your Compose account login at the very least.\r\n\r\nAfter you have clicked Add user in this screen, you'll be redirected to the Jobs display, where you should see the user being added to each of the access portals. Once that's done, it is time to connect. The next question is...\r\n\r\nThat can be answered on the Overview page. If you go there you'll find a number of \"Connect Strings\" listed in the first panel. As Elasticsearch deployments have two access portals, there's two URL's listed under the \"HTTP connection\" section.\r\n\r\nEither one of them will work – we'll just use the first one shown here for this example. We then substitute in our user name and password to get:\r\n\r\nAnd we're ready to go. Well, at least once we have an application that can use that URL. If we just want to test connectivity to the Elasticsearch cluster, we can use the example Cluster Health Call. This uses the curl command line utility which is widely available. If your operating system doesn't have curl then you can download it from http://curl.haxx.se/download.html. Then you can substitute in the user name and password and ask the cluster about its health like so:\r\n\r\n$ curl --user example:examplepass 'https://aws-us-east-1-portal5.dblayer.com:10225/_cluster/health?pretty'\r\n\r\n\"cluster_name\" : \"runstate-elasticsearch\",\r\n\r\n\"status\" : \"green\",\r\n\r\n\"timed_out\" : false,\r\n\r\n\"number_of_nodes\" : 3,\r\n\r\n\"number_of_data_nodes\" : 3,\r\n\r\n\"active_primary_shards\" : 0,\r\n\r\n\"active_shards\" : 0,\r\n\r\n\"relocating_shards\" : 0,\r\n\r\n\"initializing_shards\" : 0,\r\n\r\n\"unassigned_shards\" : 0,\r\n\r\n\"number_of_pending_tasks\" : 0\r\n\r\nNow, you may notice that we used a slightly different URL there, without the username:password embedded in it. We can use the URL we created earlier by appending /_cluster/health?pretty like so:\r\n\r\n$ curl 'https://example:examplepass@aws-us-east-1-portal5.dblayer.com:10225/_cluster/health?pretty'\r\n\r\nRemember to wrap it in quotes so that the shell doesn't see the ? and try and match files using the URL though. If you don't have curl but do have wget you can use the URL with that command like so:\r\n\r\n$ wget -O - 'https://example:examplepass@aws-us-east-1-portal5.dblayer.com:10225/_cluster/health?pretty'\r\n\r\nAnyway, assuming this worked, you have a connection to your Elasticsearch database. The \"Cluster Health\" URL is a query on Elasticsearch's REST API and you could, if you really wanted to, access it entirely from the command line. But you will probably want to go for the far less taxing route of using a library.\r\n\r\nWe'll just create a small Node.js application now to show how you can connect to your Elasticsearch cluster. Create an Node.js project with npm init and just press return to agree to all the settings. Now, run npm install elasticsearch --save to install the Elasticsearch module. Now we can write some code...\r\n\r\nvar elasticsearch=require('elasticsearch');\r\n\r\nvar client=new elasticsearch.Client( {\r\n\r\nhosts: [\r\n\r\n'https://example:examplepass@aws-us-east-1-portal5.dblayer.com:10225/',\r\n\r\n'https://example:examplepass@aws-us-east-1-portal4.dblayer.com:10216/'\r\n\r\nFirst, this code \"requires\" the elasticsearch module. Then it moves on to create an Elasticsearch client. The Client only takes one parameter, but that just is a way of wrapping up a lot of options to create connections to the Elasticsearch cluster. If you check the documentation you'll see the full extent. Here though we're keeping it simple and just passing the host key with an array of URLs to connect to as its value. The first URL is the one we've been using and the second URL is the second one from the overview's \"Connect Strings\". Let's go query the cluster health in JavaScript...\r\n\r\nclient.cluster.health({},function(err,resp,status) {\r\n\r\nconsole.log(resp);\r\n\r\nOk, we can issue the call, to get cluster health and when we get the response, we run the callback which just prints the response. The {} at the start is where the options get placed and looking at the API entry for cluster.health we can see there's some useful options to be harnessed, like the ability to wait for particular statuses or availabilities. This of course is just an example. Now we have a client we can use the Elasticsearch Quick Start examples and the rest of the documentation to master talking with Elasticsearch.\r\n\r\nThere is another way to interact with the Elasticsearch cluster and that's through the web-based site plugins. You'll find them by selecting Plugins from the side bar. There's Kibana, ElasticHQ, Bigdesk, Head, Paramedic and Kopf, which all have different capabilities, from just monitoring cluster health to helping with query creation and execution.\r\n\r\nYou're next steps with Elasticsearch should be to...\r\n\r\n* Check out the Elasticsearch: The Definitive Guide for how to master Elasticsearch's search capabilities\r\n\r\n* Find out more about the Compose Transporter which can help you get your data from other databases into your Elasticsearch database.","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:30:20 +00:00"}},{"id":"72066661343588428fd0378ef54999edb282a2bd","order":[20150803.0,31],"fields":{},"doc":{"_id":"72066661343588428fd0378ef54999edb282a2bd","_rev":"3-fa4f805dcc1e8a3f8b4c21ca7ea2be7a","created_at":"2015-08-03 14:46:42 +00:00","full_name":"Getting Started with Compose and Bluemix","url":"https://developer.ibm.com/bluemix/2015/07/29/getting-started-compose-bluemix/","status":"Live","name":"Getting Started with Compose and Bluemix","description":"This article walks you through a simple example of how to use Compose’s DBaaS offerings within IBM Bluemix, IBM’s developer cloud platform. ","languages":["Python"],"technologies":["Bluemix","Redis"],"topic":[],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:24:28 +00:00"}},{"id":"80dfbfa3fb52ed4fbc5cfbea934f9f2588c75557","order":[20150803.0,36],"fields":{},"doc":{"_id":"80dfbfa3fb52ed4fbc5cfbea934f9f2588c75557","_rev":"5-e469234c04926885b406cb2adb9f3b1d","created_at":"2015-08-03 14:38:47 +00:00","full_name":"Analyzing Salesforce Data with Looker Unblock your CRM data, and make data-driven decisions with your whole team!","url":"https://developer.ibm.com/clouddataservices/simple-data-pipe/","status":"Live","name":"Analyzing Salesforce Data with Looker : A Salesforce BI solution with dashDB, Cloudant, DataWorks, and Looker","description":"The Simple Data Pipe is an app that moves your Salesforce data to dashDB, which is the IBM cloud data warehouse. Once you have your Salesforce data in dashDB, you can do all kinds of analysis on it, with all kinds of tools, such as SQL, R, and Looker.","languages":["Node.js","AngularJS"],"technologies":["dashDB","Looker","Cloudant","Salesforce","DataWorks","OAuth","WebSockets"],"topic":["Data Warehousing","Analytics","Migration"],"type":"Tutorial","level":"Intermediate","imageurl":"http://developer.ibm.com/clouddataservices/wp-content/uploads/sites/47/2015/07/simple-data-pipe.png","body":"","githuburl":"https://github.com/ibm-cds-labs/pipes","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-10 19:03:57 +00:00"}},{"id":"47ed002fdf5e12f0ec2ac4325ae1300b22583e22","order":[20150731.0,25],"fields":{},"doc":{"_id":"47ed002fdf5e12f0ec2ac4325ae1300b22583e22","_rev":"3-0b2337e925a2ec55d691bf5384c61a2a","created_at":"2015-07-31 16:28:16 +00:00","full_name":"Elasticsearch Tools & Compose","url":"https://www.compose.io/articles/elasticsearch-tools-and-compose/","status":"Live","name":"Elasticsearch Tools & Compose","description":"There's a world of tools that make the Elasticsearch even more useful and accessible. In this article we'll look at some and show what you do to get them working with Compose's Elasticsearch deployments. ","languages":["HTTP","Java","JavaScript"],"technologies":["Elasticsearch"],"topic":[],"type":"Article","level":"Beginner","imageurl":"","body":"Outside of the core Elasticsearch toolset, there's a world of tools that make the search and analytics database even more useful and accessible. In this article we'll look at some and show what you do to get them working with Compose's Elasticsearch deployments. We'll start with a command line tool, move on to a simple search tool and finish with an all purpose client for searching and manipulating your Elasticsearch database...\r\n\r\nLet us start the tool tour with Es2unix, from the Elasticsearch developers. Es2unix is a version of the Elasticsearch API that you can use from the command line. It doesn't just make the API calls though, it also converts the returned results into a line-oriented, tabular format like many other Unix tools output. That makes it ideal for integrating Elasticsearch into your awk, grep and sort using shell scripts.\r\n\r\nEs2unix will need Java installed, Java 7 at least, and the binary version can be simply downloaded with a curl command and enabled with chmod as per the installation instructions:\r\n\r\ncurl -s download.elasticsearch.org/es2unix/es >~/bin/es\r\n\r\nchmod +x ~/bin/es\r\n\r\nNote this assumes you have a bin directory in your $HOME and it's on your path.\r\n\r\nNow, when you run es it'll assume that Elasticsearch is running locally. When you are using Compose Elasticsearch, that isn't the case. If you've got the HTTP/TCP access portal enabled, you'll have to give the es command a URL to locate your Elasticsearch deployment. You can get the URL from your Compose dashboard - remember to substitute in the username and password of a Elasticsearch user (from the Users tab) into the URL. This URL is then passed using the -u option:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ version\r\n\r\nes            20140723711d4f9\r\n\r\nelasticsearch 1.3.4\r\n\r\nThe es command is followed by one of a selection of subcommands. There we've used the version subcommand to get the version of the es command and the version of Elasticsearch it is talking to. The health of the cluster can be established with the health subcommand:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ health -v\r\n\r\ntime     cluster    status nodes data pri shards relo init unassign\r\n\r\n11:14:39 EsExemplum green      3    3   3      6    0    0        0\r\n\r\nDrop the -v to get unlabelled results, ideal for passing into monitoring software - adding -v on many es subcommands is a signal that more extensive labelling of returned data is desired.\r\n\r\nThe es command has the ability to count all documents or the number of documents that meets a simple query, and to search all indices and return matching ids:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ count \"one species or variety\"\r\n\r\n11:44:02 16 \"one species or variety\"\r\n\r\nshows a count of documents matching the parts of that phrase to different extents. Using the search command we can dig deeper:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ -v search \"one species or variety\"\r\n\r\nscore   index         type    id\r\n\r\n0.16337 darwin-origin chapter II\r\n\r\n0.12559 darwin-origin chapter IX\r\n\r\n0.10360 darwin-origin chapter IV\r\n\r\n0.10141 darwin-origin chapter I\r\n\r\n0.09734 darwin-origin chapter XI\r\n\r\n0.09326 darwin-origin chapter V\r\n\r\n0.09226 darwin-origin chapter XV\r\n\r\n0.08744 darwin-origin chapter XIV\r\n\r\n0.08069 darwin-origin chapter VIII\r\n\r\n0.07525 darwin-origin chapter III\r\n\r\nTotal: 16\r\n\r\nNow we can see the matching score along with the id, index and type of the document. Although here, 16 documents match, Elasticsearch returns only the top ten results by default. If we wanted to be more precise  we could quote the string (remembering we're in the shell so back-slash escapes are needed) and select a field for matching:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ -v search \"\"one species or variety\"\" text\r\n\r\nscore   index         type    id text\r\n\r\n0.03073 darwin-origin chapter I  [\"CHAPTER I. VARIATI\r\n\r\n0.03073 darwin-origin chapter IX [\"CHAPTER IX. HYBRID\r\n\r\nTotal: 2\r\n\r\nOther subcommands in es2unix include indices, for listing indexes, ids for retrieving all ids from an index and a variety of management reporting commands such as nodes, heap and shards.\r\n\r\nYou'll have probably noticed that the es command is a little laborious when you have to specify the URL every time. Es2unix doesn't have any short cuts when it comes to passing that URL like environment variables. There is another way though to shorten things and thats by using an SSH access portal instead. If you configure an SSH access portal for your Elasticsearch deployment then the default command for creating your SSH tunnels makes a node of the cluster appear to be at localhost:9200 which is the default. Once you have an SSH tunnel set up, you can drop the entire -u [URL] part and use tools as if you had Elasticsearch locally configured.\r\n\r\nSometime you just want to set up a quick search for your Elasticsearch database with the minimum of effort. The Calaca project is very useful in that regard. It's an all JavaScript search front end for Elasticsearch which connects up to Elasticsearch. To get up and running, you'll want to download and unpack the zip file available from the Github page. Calaca's configuration can be found in the file js/config.js which looks like this:\r\n\r\nvar indexName = \"name\"; //Ex: twitter\r\n\r\nvar docType = \"type\"; //Ex: tweet\r\n\r\nvar maxResultsSize = 10;\r\n\r\nvar host = \"localhost\"; //Ex: http://ec2-123-aws.com\r\n\r\nvar port = 9200;\r\n\r\nAs you can see, it comes configured to use the database on localhost port 9200, so you could use the SSH shortcut above. But we're here anyway so we need to change the host variable to \"https://user:pass@haproxy1.dblayer.com\" to match the URL we're given in the Compose dashboard and don't forget to copy in the username and password. The port number also needs to be copied from the dashboard URL to the port variable. The rest of the configuration is selecting what to search and what to show. Set the indexName and docType variables to index and data type you want to search. So, for our example here we have a config.js that reads:\r\n\r\nvar indexName = \"darwin-origin\";\r\n\r\nvar docType = \"chapter\";\r\n\r\nvar maxResultsSize = 10;\r\n\r\nvar host = \"https://user:pass@haproxy1.dblayer.com\";\r\n\r\nvar port = 10361;\r\n\r\nThen it's a matter of editing the index.html file to set what results are shown. In the middle of the file is a section which says:\r\n\r\nEdit the result.name and result.description to display what fields you want to display from your document:\r\n\r\nWe have a particularly long block of text in our document which we truncates down and we use the id and title together to create a heading. Save that, open index.html in your browser – there's no need to deploy to a server – and you'll see Calaca's search field. Enter a term and you'll see results like so:\r\n\r\nIt's a quick way to get a pretty search query front end up locally without wrestling with forming Curl/JSON requests or deploying a full on server.\r\n\r\nWhere Calaca's great for a super simple search client, you might want something a little more potent for your searching. For that, try ESClient, which not only has an extensive search UI but adds the ability to display those results in a table or as raw JSON results and then edit and delete selected documents. Like Calaca, ESClient needs no server, just download the zip or clone the Github respository.  Configuring it means just editing the config.js file and putting in the URL from the Compose dashboard:\r\n\r\nvar Config = {\r\n\r\n'CLUSTER_URL':'https://user:pass@haproxy1.dblayer.com:10361',\r\n\r\nThen you open esQueryClient.html in your browser and before you know it, there's the ESClient configuration screen - click the Connect button and a connection to the Elasticsearch database will be made and you'll be moved to the Search tab where you can select index, type, fields, sort fields, specify a Lucene or DSL query and click Search to see the results in a table below the query.\r\n\r\nDouble clicking on a result will let you edit the documents that make up the result or you can use the results as a guide for a delete operation. If you set to \"Raw JSON\" switch in the Configuration tab, you'll also be able to view the complete raw returned results in the JSON Results tab.\r\n\r\nIt's all rather usefully functional and there's only one slight problem. If you look at the top of the ESClient page, you'll see it's displaying the username and password as part of the URL for the database you are connecting to. Not really ideal that, but the SSH access portal can help out there too. If you set up and activate the tunnel, then you can return the CLUSTER_URL value in the config.js file to http://localhost:9200 and there'll be no username or password to display on screen.\r\n\r\nWe've touched on three tools in this article, but more importantly we've shown the practical differences between using the HTTP/TCP and SSH access portals on componse. With HTTP/TCP access, there will be usernames and passwords embedded in the URL you use and this will leave any scripts or tools you configure susceptible to shoulder surfers and the like. That said, for occasionally launched tools it is quick and simple.\r\n\r\nWith the SSH access portal, the configuration and authentication is done when you set up the tunnel in a separate process and the tunnel means you can use Elasticsearch as if the node was installed locally. The downside is you do need to make sure the SSH tunnel is up before you run any command and it may be easier to go through the HTTP/TCP access portal. But then thats why we give you both options at Compose so you can choose what suits you and your applications best.","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:24:01 +00:00"}},{"id":"6449d51ed4936ed705365837e21bcabbe52158f0","order":[20150731.0,26],"fields":{},"doc":{"_id":"6449d51ed4936ed705365837e21bcabbe52158f0","_rev":"5-84bf980ca9057aa19299b1170265ba27","created_at":"2015-07-31 15:46:19 +00:00","full_name":"MongoDB 2.6 Shell & Tools","url":"https://www.compose.io/articles/mongodb-2-6-shell-and-tools-things-worth-knowing/","status":"Live","name":"MongoDB 2.6 Shell & Tools","description":"Customize your Mongo shell experience","languages":["JavaScript"],"technologies":["MongoDB"],"topic":[],"type":"Article","level":"Intermediate","imageurl":"","body":"Earlier this year, as part of a series on the Mongo Shell, we introduced readers to the .mongorc.js file. This file, which lives in a user's home directory, is automatically executed when the mongo shell is run, making it a great place to customise your Mongo shell experience. You can, for example, load it with time-saving functions and pre-initialised variables. In MongoDB 2.6 there's now another startup file, /etc/mongorc.js, which is a global shell startup file that is run before the user's own .mongorc.js file. If a number of users share various tools in the shell, it's really useful to have them in /etc/mongorc.js, as long as everyone has read permission for that file of course.\r\n\r\nOne thing you can't do is stop the global file from being run; the user's own startup file can be disabled with --norc as an option to mongo, but that doesn't affect the global file. There's two things this means. First, the global file is also a good place to put commands you want people to use and not have the excuse they didn't realise they weren't loading them. Secondly, you need to make really sure that your /etc/mongorc.js is valid and correct.\r\n\r\nOutside the shell one of the smaller changes could save you a few keystrokes or cost you a few key if you don't know about it. The mongoimport command takes as an option --collection (also -c), which lets you specify which collection the imported data is going into – in 2.4 and before it was pretty much mandatory. In MongoDB 2.6, you can drop the --collection option and the importer will use the name of the file you are importing from as the name of the collection. So, you can save keystrokes by exporting your data to a file with the collection name its destined for (or be caught out when you accidentally forget the -c option and wonder where that new collection in your database came from).\r\n\r\nIf you've ever felt that the MongoDB executables were somewhat noisy, the --quiet option may bring some calm to your day. In 2.6 the  --quiet option, which suppresses all logging messages except errors, is supported by all the MongoDB command-line tools rather than a select few.","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:24:15 +00:00"}},{"id":"a3f8f63753e3e2c6d4cc6c78369edb68c0d0647e","order":[20150731.0,27],"fields":{},"doc":{"_id":"a3f8f63753e3e2c6d4cc6c78369edb68c0d0647e","_rev":"3-f100d43f0cc9855a2ae08ff5b7ef748f","created_at":"2015-07-31 15:28:57 +00:00","full_name":"SQLPro for Postgres and Keylord for Redis","url":"https://www.compose.io/articles/tooltime-sqlpro-for-postgres-and-keylord-for-redis/","status":"Live","name":"SQLPro for Postgres and Keylord for Redis","description":"We review 2 great tools for working with your databases.","languages":["SQL"],"technologies":["PostgreSQL","Redis"],"topic":["SQL"],"type":"Article","level":"Beginner","imageurl":"","body":"At Compose we're always on the lookout for new ways to work with your databases. Database administration and browsing tools are especially popular. After we ran our Compose data browsing guide we've come across two new contenders which work with PostgreSQL and Redis. So we thought we would drill down on SQLPro for Postgres and Keylord...\r\n\r\nIn the world of PostgreSQL, there's only been a few client side data browsers. That's in part down to the existence of PGadmin3, the all-singing Swiss Army knife of admin tools. But its strength is also its weakness as there's so much functionality in there that it's like getting handed the Swiss Army knife with all the blades out. So there's been a move to make simpler, data-centric PostgreSQL browsing applications. And thats where SQLPro for Postgres comes in.\r\n\r\nSQLPro is a Mac OS X only application with a very neat and deceptively simple front end concealing a lot of functionality – it's very much a classic Mac application. On the left hand side of the window, it shows a tree, rooted in connections to PostgreSQL database instances with databases as children. Each database has just three children, Tables, Views and Functions letting you get at the most commonly created and modified elements of a Postgres database. The right hand side of the window is a tabbed editor/results viewer.\r\n\r\nThe editor is an autocompleting, SQL syntax highlighting edit whose contents will be run on clicking the \"Execute All\" button. The results appear in a multiple table view below the editor which, when multiple result sets are returned by the executed SQL, uses slidable paned tables in the view. This is great for, for example, comparing results of two queries. Queries can also be saved locally and the results view is also an in-place editor for the data values.\r\n\r\nIt's not just tables you can do this with, views can be edited and their results browsed too. Tables and views also expand out in the tree to present their component fields, complete with type and default information and key fields marked with a key.\r\n\r\nSQLPro does have the un-Mac habit is putting most of the editing functions in context menus on the tree view, tucked away. Want to modify a function? Select 'Alter Function' from its context menu. Want to drop a column? 'Drop Column' is in each column's context menu. Want to create or modify a table? \"New Table\" and \"Alter Table\" are in the context menu for the database and all the tables. Table modification is one time the right hand side UI changes, away from an editor to two tables of columns and indexes and one of the more competent table creation interfaces around though it could do with giving more assistance in selecting column types.\r\n\r\nAnd there's still more features to make scripting your database easier by creating scripts for particular functions. And where you have two or more databases open, you can switch between them, applying the same SQL between them.\r\n\r\nSQLPro for Postgres is one of the most focussed and competent SQL database browsers we've seen here and it is definitely worth a look. There's a downloadable version and currently the browser is selling for $6.\r\n\r\nDesigned for key-value databases like Redis (and LevelDB), Keylord is across-platform (Windows, Mac OS X and Linux) application which sets out to give a multi-column browser view of the Redis key-value store. All it needs to connect is the host, port and password for Redis installations that are visible on the web, though it also has support for connections over SSH tunnels.\r\n\r\nOnce connected, you can see and filter the key list on the database and click in on any of the keys to see their contents. The UI supports examining and editing of Hash, List, Set, Ordered Set and String keys. It also supports the HyperHyperLog value though, due to its being a rather clever data structure for counting unique values, only takes a string value entered yet shows you the number of unique values that it has seen. As well as editing existing key-value pairs, it can also create new key-values with the same selection of types as it can browse. For some fields there's also the option of a hex view of the value, making Keylord more useful if files are stored in the database.\r\n\r\nAnd that's about it. For each different database on a Redis instance you have to create a new connection. Performance in downloading the keys is good but there's no key sorting to make them easier to browse.  There's also no TTL setting or display and the list views require manual refreshing - if you have two tabs open on the same database and delete items in one tab, you need to remember to refresh the other tab before using it. There's also a lack of commands for administering the database.\r\n\r\nIt's a solid foundation for what could be a fine tool for Redis users but you are likely to be running it alongside the Redis CLI for any database administration or debugging. Keylord costs $29 for a personal license and $49 per seat for the enterprise licence. There is a downloadable trial version if you want to try it out.","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:25:56 +00:00"}}],"counts":{"language":{"Android":1.0,"AngularJS":1.0,"C#":1.0,"CSS":2.0,"Go":1.0,"HTML5":5.0,"HTTP":13.0,"Java":7.0,"JavaScript":30.0,"Node.js":8.0,"Objective-C":3.0,"PHP":2.0,"Python":6.0,"R":11.0,"Ruby":3.0,"SQL":12.0,"Swift":1.0,"iOS":4.0},"level":{"Advanced":4.0,"Beginner":105.0,"Intermediate":50.0},"technology":{"Bluemix":23.0,"Cloudant":99.0,"Cloudant Local":1.0,"Cordova":1.0,"CouchDB":4.0,"DataWorks":4.0,"Elasticsearch":3.0,"Graph Data Store":1.0,"Ionic":1.0,"Looker":1.0,"MobileFirst":7.0,"MongoDB":8.0,"OAuth":1.0,"PhoneGap":1.0,"PostgreSQL":7.0,"PouchDB":5.0,"Redis":4.0,"Salesforce":1.0,"Spark":1.0,"WebSockets":1.0,"dashDB":43.0},"topic":{"Analytics":31.0,"Data Warehousing":36.0,"Gaming":1.0,"Hybrid":5.0,"IoT":6.0,"Location":15.0,"Migration":15.0,"Mobile":18.0,"NoSQL":83.0,"Offline":11.0,"Open Data":2.0,"SQL":6.0,"Standards":2.0},"type":{"Article":72.0,"Tutorial":32.0,"Video":55.0}}}
