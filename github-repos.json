{"total_rows":159,"bookmark":"g1AAAADaeJzLYWBgYM5gTmGQT0lKzi9KdUhJMtRLykxPyilN1UvOyS9NScwr0ctLLckBKmRKZEiy____fxaY4-ZYbD5nA5CVKJ2FaoIxbhPyWIAkQwOQApqzn5BBRoQMOgAxCMlFCx-ADNLPygIAtHRDqw","rows":[{"id":"fc60323477506db4b0f6424eb4b73202eea6903d","order":[20151001.0,37],"fields":{},"doc":{"_id":"fc60323477506db4b0f6424eb4b73202eea6903d","_rev":"2-42040dc17fa73d97ac4cbe813df55d5d","created_at":"2015-10-01 21:13:27 +00:00","full_name":"Get started with dashDB on Bluemix","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/get/get-started-with-dashdb-on-bluemix/","status":"Live","name":"Watch how to get started with dashDB on Bluemix","description":"See how quick and easy it is to set up a dashDB instance in IBM Bluemix and load data to perform analytics in dashDB.","languages":[],"technologies":["Bluemix","dashDB"],"topic":["Analytics","Data Warehousing"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=-ar3xfg0uGw","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:13:27 +00:00"}},{"id":"60a491d4e7c167ad8a38ac6595dd0e56bd5b44f6","order":[20151001.0,38],"fields":{},"doc":{"_id":"60a491d4e7c167ad8a38ac6595dd0e56bd5b44f6","_rev":"2-ef8bd8c70979bafa17ca7690d7d0d0a4","created_at":"2015-10-01 19:49:08 +00:00","full_name":"Cloud Data Services","url":"https://developer.ibm.com/clouddataservices/docs/dataworks/","status":"Live","name":"Introduction to DataWorks","description":"IBM DataWorks mixes and refines raw data from both on-premises and off-premises environments. ","languages":[],"technologies":["DataWorks"],"topic":["Migration"],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://youtu.be/C4XnZSkCDxs","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 19:49:08 +00:00"}},{"id":"e1ea18968e3b8692f607cc9c56f8d383165799e7","order":[20151001.0,38],"fields":{},"doc":{"_id":"e1ea18968e3b8692f607cc9c56f8d383165799e7","_rev":"2-5b87ec7f23904b0fd2de5146790fe022","created_at":"2015-10-01 21:22:06 +00:00","full_name":"Extract and export dashDB data to a CSV file","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/analyze/extract-and-export-dashdb-data-to-a-csv-file/","status":"Live","name":"Extract and export dashDB data to a CSV file","description":"Watch how to extract and export dashDB data to a CSV file for import into a spreadsheet application such as Microsoft Excel or Open Office. ","languages":[],"technologies":["dashDB"],"topic":["Analytics","Migration"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=KD3cbTfmzUc","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:22:06 +00:00"}},{"id":"01030c4838ae298ea2f671eda79e2365feffff63","order":[20151001.0,39],"fields":{},"doc":{"_id":"01030c4838ae298ea2f671eda79e2365feffff63","_rev":"2-35e65c36fabf460b7ef4114d28222b37","created_at":"2015-10-01 20:42:32 +00:00","full_name":"Cloud Data Services","url":"https://developer.ibm.com/clouddataservices/docs/dataworks/dataworks-forge/","status":"Live","name":"IBM DataWorks Forge","description":"Load data from data sources like SQL Database and dashDB, understand it's quality, refine the data, and then deliver the refined data to systems and applications.","languages":[],"technologies":["DataWorks"],"topic":["Migration"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"http://ibmtvdemo.edgesuite.net/software/dataworks/getting-started/getting-started-with-dataworks-forge.html","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 20:42:32 +00:00"}},{"id":"2f362e3286ac1d4ebe1dd58af02b4de14bf0bd6d","order":[20151001.0,40],"fields":{},"doc":{"_id":"2f362e3286ac1d4ebe1dd58af02b4de14bf0bd6d","_rev":"2-4f594fcfe2ff19a74c2117cdd3c79810","created_at":"2015-10-01 21:16:39 +00:00","full_name":"Load XML data into dashDB","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/get/load-xml-data-into-dashdb/","status":"Live","name":"Load XML data into dashDB","description":"Watch how to convert XML data to CSV format to load into dashDB. This video shows a tool called Convert XML to CSV found here: http://www.convertcsv.com/xml-to-csv.htm","languages":[],"technologies":["dashDB"],"topic":["Migration"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=xQqsKz7shUI","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:16:39 +00:00"}},{"id":"d456fef4a7e79eae124d585eaad414d594e48d78","order":[20151001.0,41],"fields":{},"doc":{"_id":"d456fef4a7e79eae124d585eaad414d594e48d78","_rev":"2-1d417de237cdb56457e06d212cfb4bb4","created_at":"2015-10-01 21:18:12 +00:00","full_name":"Perform market basket analysis using dashDB and R","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/analyze/perform-market-basket-analysis-using-dashdb-and-r/","status":"Live","name":"Perform market basket analysis using dashDB and R","description":"Watch how to apply association rules using R to data stored in dashDB using a retail market basket scenario. ","languages":["R"],"technologies":["dashDB"],"topic":["Analytics"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=U4cATqOvmH0","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:18:12 +00:00"}},{"id":"a6bd327f50b77583443fb9ece9890d735081600f","order":[20151001.0,52],"fields":{},"doc":{"_id":"a6bd327f50b77583443fb9ece9890d735081600f","_rev":"2-de9f172e95583f1c30e1daeb82634030","created_at":"2015-10-01 21:15:04 +00:00","full_name":"Store Tweets Using Bluemix, Node-RED, Cloudant, and dashDB","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/get/store-tweets-using-bluemix-node-red-cloudant-and-dashdb/","status":"Live","name":"Store Tweets Using Bluemix, Node-RED, Cloudant, and dashDB","description":"Watch how to create a Bluemix application using Node-RED that searches for tweets and stores the resulting tweets in a Cloudant NoSQL database, then use the data warehousing tool built into Cloudant to load the data into a dashDB.","languages":[],"technologies":["Bluemix","Cloudant","dashDB"],"topic":["Data Warehousing"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=QSnzz3pKDNE","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:15:04 +00:00"}},{"id":"cae9e7234d5cbd38e421df0923bb545e8c6b0dd7","order":[20151001.0,53],"fields":{},"doc":{"_id":"cae9e7234d5cbd38e421df0923bb545e8c6b0dd7","_rev":"2-febfc3bcd94a9ea3ec1b1344b514a148","created_at":"2015-10-01 21:19:32 +00:00","full_name":"Leverage dashDB in Cognos Business Intelligence","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/analyze/leverage-dashdb-in-cognos-business-intelligence/","status":"Live","name":"Leverage dashDB in Cognos Business Intelligence","description":"Learn how to configure a dashDB connection in Cognos Business Intelligence and create stunning visualizations. ","languages":[],"technologies":["dashDB"],"topic":["Analytics"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=EAr8sWEtx6A","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:19:32 +00:00"}},{"id":"4413de0a2ef4c74a6d301b97d4e4bfadaf5e0ec9","order":[20151001.0,54],"fields":{},"doc":{"_id":"4413de0a2ef4c74a6d301b97d4e4bfadaf5e0ec9","_rev":"2-e3df19977f506f8a5d711af20feca75e","created_at":"2015-10-01 21:20:52 +00:00","full_name":"Integrate dashDB with Excel","url":"https://developer.ibm.com/clouddataservices/docs/dashdb/analyze/integrate-dashdb-with-excel/","status":"Live","name":"Integrate dashDB with Excel","description":"Love to work in Microsoft Excel? Watch how to connect to IBM dashDB as the data source for Excel, and how to import tables into a spreadsheet. ","languages":[],"technologies":["dashDB"],"topic":["Analytics"],"type":"Video","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"https://www.youtube.com/watch?v=oRuN9bniAxU","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-10-01 21:20:52 +00:00"}},{"id":"bbd45ab50483e6d7d1c0ed4ee0f629d15a111162","order":[20150922.0,37],"fields":{},"doc":{"_id":"bbd45ab50483e6d7d1c0ed4ee0f629d15a111162","_rev":"3-e33d92bf3ebc3a2a9aed782e3ab7b675","created_at":"2015-09-22 18:28:19 +00:00","full_name":"Export Cloudant JSON as CSV, RSS, or iCal","url":"https://developer.ibm.com/clouddataservices/2015/09/22/export-cloudant-json-as-csv-rss-or-ical/","status":"Live","name":"Export Cloudant JSON as CSV, RSS, or iCal","description":"Need to convert your Cloudant JSON to a different format? Read how to get JSON data into a spreadsheet, into your calendar, or aggregate data in an RSS reader.","languages":[],"technologies":["Cloudant"],"topic":[],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-09-22 18:28:48 +00:00"}},{"id":"fc97e099d61423e3fe13bd977e2149ad85f44a78","order":[20150824.0,48],"fields":{},"doc":{"_id":"fc97e099d61423e3fe13bd977e2149ad85f44a78","_rev":"3-55a98b5c86a763af7487f84e87c7ff66","created_at":"2015-08-24 17:21:10 +00:00","full_name":"Your First Data Warehouse Is Easy. Meet the ODS.","url":"https://developer.ibm.com/clouddataservices/2015/08/24/operational-data-store/","status":"Live","name":"Your First Data Warehouse Is Easy. Meet the ODS.","description":"Building your first data warehouse doesn’t have to be as enterprise-y and scary as it sounds, especially in the era of cloud services. A good first iteration of your warehousing environment is a simple architecture that we BI architects like to call an “Operational Data Store” (ODS).","languages":[],"technologies":["dashDB"],"topic":["Data Warehousing"],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-24 17:21:30 +00:00"}},{"id":"36f6d18d91505d767abd3f93754dda722164e38c","order":[20150824.0,49],"fields":{},"doc":{"_id":"36f6d18d91505d767abd3f93754dda722164e38c","_rev":"2-628c7e74a22631266f0f2ab5816d313b","created_at":"2015-08-24 17:25:21 +00:00","full_name":"Speed your SQL Queries with Spark SQL","url":"https://developer.ibm.com/clouddataservices/2015/08/19/speed-your-sql-queries-with-spark-sql/","status":"Live","name":"Speed your SQL Queries with Spark SQL","description":"Get faster queries and write less code too. Learn how to use Spark SQL to query your relational database. Follow this tutorial and see how to query a cloud-based Compose PostgreSQL instance or a local PostreSQL database.","languages":["SQL"],"technologies":["Spark","PostgreSQL"],"topic":["SQL"],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-24 17:25:21 +00:00"}},{"id":"425e13adcde45fef1910b32895137ba3cae4be5b","order":[20150824.0,50],"fields":{},"doc":{"_id":"425e13adcde45fef1910b32895137ba3cae4be5b","_rev":"3-e64f87e5baada158a04d44df3253a008","created_at":"2015-08-24 17:58:20 +00:00","full_name":"How to analyze your pipe runs with Bunyan","url":"https://developer.ibm.com/clouddataservices/2015/08/11/analyze-pipe-runs-bunyan/","status":"Live","name":"How to analyze your pipe runs with Bunyan","description":"How to use Bunyan to capture detailed logging of data migration runs through our Simple Data Pipes app.","languages":[],"technologies":["Cloudant"],"topic":["Migration","NoSQL"],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-24 17:58:41 +00:00"}},{"id":"476f784bbdab5efe54a2ed5e74f39504e7531395","order":[20150814.0,47],"fields":{},"doc":{"_id":"476f784bbdab5efe54a2ed5e74f39504e7531395","_rev":"3-f1b1edc5204cead6d7ac4fbae96225d3","created_at":"2015-08-14 09:57:26 +00:00","full_name":"Writing Data Directly to Cloudant from Slack","url":"https://developer.ibm.com/clouddataservices/2015/08/13/writing-data-directly-cloudant-slack/","status":"Live","name":"Writing Data Directly to Cloudant from Slack","description":"Slack's integration API allows external services to be plugged in with ease. Even if your service isn't listed in the off-the-shelf integrations, you can still push data to other HTTP services. This tutorial shows how a Slack 'slash command' can be configured to push data to a Cloudant or CouchDB database in a few easy steps.","languages":["JavaScript"],"technologies":["Cloudant"],"topic":["NoSQL"],"type":"Tutorial","level":"Beginner","imageurl":"http://developer.ibm.com/clouddataservices/wp-content/uploads/sites/47/2015/08/sl_2.png","body":"Slack’s Integration API and Cloudant’s HTTP API make it simple to store data directly into a Cloudant database without breaking a sweat. This tutorial shows how to create a custom slash command in Slack and how to post it directly to Cloudant.\r\n\r\nSlack is a messaging and team-working application that is used widely to allow disparate teams of people to chat, share files, and interact on desktop, tablet, and mobile platforms. We use Slack in IBM Cloud Data Services to coordinate our activities, to work in an open collaborative environment, and to cut down on email and meetings.\r\n\r\nOne of the strengths of Slack is that it integrates with other web services, so events happening in Github or Stack Overflow can be surfaced in the appropriate Slack channels. Slack also has an API that lets you create custom integrations. The simplest of these is slash commands: when a user starts a Slack message with a forward slash followed by a command string, Slack can be configured to POST that data to an external API. Say you create the slash command /lunch. A user could type:","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-14 09:57:35 +00:00"}},{"id":"c716564ddbd0e085ab3ee9af11552fc209a60994","order":[20150804.0,36],"fields":{},"doc":{"_id":"c716564ddbd0e085ab3ee9af11552fc209a60994","_rev":"3-22e53c15e79692a2bb882c65615e404e","created_at":"2015-08-04 15:26:08 +00:00","full_name":"Getting Started with Elasticsearch using Compose","url":"https://www.compose.io/articles/getting-started-with-elasticsearch-using-compose/","status":"Live","name":"Getting Started with Elasticsearch using Compose","description":"How to set up an Elasticsearch database on Compose.","languages":["HTTP","JavaScript"],"technologies":["MongoDB"],"topic":[],"type":"Article","level":"Beginner","imageurl":"","body":"Hi again! In this latest part of our tour through getting started with the multiple, or singular, database joys of Compose we'll be looking at setting up Elasticsearch. In the first part, we covered MongoDB on Compose and the sign up process is basically the same for both. If you didn't read that article, have a browse now...\r\n\r\nThere is of course one difference, obviously you'll need to select Elasticsearch in the Choose a Database panel and take a note that the Elasticsearch pricing is different to MongoDB's. If you're on the thirty day trial, that won't bother you immediately, but do take a note.\r\n\r\nAnyway, back to the Elasticsearch database setup. Once you have signed in, you'll be greeted by the Jobs screen showing that Compose has created your Elasticsearch database.\r\n\r\nYou're next stop should be to select the Overview option in the  sidebar. This will show you various, useful, items of information at the top, but, right now, it'll be showing a message at the top of the page...\r\n\r\nYes, but what you've got is an account which you can log in to with your email address and password for the Compose Dashboard and your own Compose account. Those credentials are only ever used to log into the Compose Dashboard. As we explain in the MongoDB walkthrough, each database has it's own credential system to be managed. For Elasticsearch those credentials are usernames and passwords for the HTTP/TCP access portal which protects your Elasticsearch cluster.\r\n\r\nThere's actually two HTTP/TCP access portals to enable redundancy in accessing the cluster and both get automatically updated with the same credentials. All you need to do is create your user/password pair and you can  connect to either of the portals (or both if your driver supports multiple URLs to connect with). Click on Users in the left hand sidebar and then click Add user.\r\n\r\nYou'll be prompted to enter a username and password here; make them different from your Compose account login at the very least.\r\n\r\nAfter you have clicked Add user in this screen, you'll be redirected to the Jobs display, where you should see the user being added to each of the access portals. Once that's done, it is time to connect. The next question is...\r\n\r\nThat can be answered on the Overview page. If you go there you'll find a number of \"Connect Strings\" listed in the first panel. As Elasticsearch deployments have two access portals, there's two URL's listed under the \"HTTP connection\" section.\r\n\r\nEither one of them will work – we'll just use the first one shown here for this example. We then substitute in our user name and password to get:\r\n\r\nAnd we're ready to go. Well, at least once we have an application that can use that URL. If we just want to test connectivity to the Elasticsearch cluster, we can use the example Cluster Health Call. This uses the curl command line utility which is widely available. If your operating system doesn't have curl then you can download it from http://curl.haxx.se/download.html. Then you can substitute in the user name and password and ask the cluster about its health like so:\r\n\r\n$ curl --user example:examplepass 'https://aws-us-east-1-portal5.dblayer.com:10225/_cluster/health?pretty'\r\n\r\n\"cluster_name\" : \"runstate-elasticsearch\",\r\n\r\n\"status\" : \"green\",\r\n\r\n\"timed_out\" : false,\r\n\r\n\"number_of_nodes\" : 3,\r\n\r\n\"number_of_data_nodes\" : 3,\r\n\r\n\"active_primary_shards\" : 0,\r\n\r\n\"active_shards\" : 0,\r\n\r\n\"relocating_shards\" : 0,\r\n\r\n\"initializing_shards\" : 0,\r\n\r\n\"unassigned_shards\" : 0,\r\n\r\n\"number_of_pending_tasks\" : 0\r\n\r\nNow, you may notice that we used a slightly different URL there, without the username:password embedded in it. We can use the URL we created earlier by appending /_cluster/health?pretty like so:\r\n\r\n$ curl 'https://example:examplepass@aws-us-east-1-portal5.dblayer.com:10225/_cluster/health?pretty'\r\n\r\nRemember to wrap it in quotes so that the shell doesn't see the ? and try and match files using the URL though. If you don't have curl but do have wget you can use the URL with that command like so:\r\n\r\n$ wget -O - 'https://example:examplepass@aws-us-east-1-portal5.dblayer.com:10225/_cluster/health?pretty'\r\n\r\nAnyway, assuming this worked, you have a connection to your Elasticsearch database. The \"Cluster Health\" URL is a query on Elasticsearch's REST API and you could, if you really wanted to, access it entirely from the command line. But you will probably want to go for the far less taxing route of using a library.\r\n\r\nWe'll just create a small Node.js application now to show how you can connect to your Elasticsearch cluster. Create an Node.js project with npm init and just press return to agree to all the settings. Now, run npm install elasticsearch --save to install the Elasticsearch module. Now we can write some code...\r\n\r\nvar elasticsearch=require('elasticsearch');\r\n\r\nvar client=new elasticsearch.Client( {\r\n\r\nhosts: [\r\n\r\n'https://example:examplepass@aws-us-east-1-portal5.dblayer.com:10225/',\r\n\r\n'https://example:examplepass@aws-us-east-1-portal4.dblayer.com:10216/'\r\n\r\nFirst, this code \"requires\" the elasticsearch module. Then it moves on to create an Elasticsearch client. The Client only takes one parameter, but that just is a way of wrapping up a lot of options to create connections to the Elasticsearch cluster. If you check the documentation you'll see the full extent. Here though we're keeping it simple and just passing the host key with an array of URLs to connect to as its value. The first URL is the one we've been using and the second URL is the second one from the overview's \"Connect Strings\". Let's go query the cluster health in JavaScript...\r\n\r\nclient.cluster.health({},function(err,resp,status) {\r\n\r\nconsole.log(resp);\r\n\r\nOk, we can issue the call, to get cluster health and when we get the response, we run the callback which just prints the response. The {} at the start is where the options get placed and looking at the API entry for cluster.health we can see there's some useful options to be harnessed, like the ability to wait for particular statuses or availabilities. This of course is just an example. Now we have a client we can use the Elasticsearch Quick Start examples and the rest of the documentation to master talking with Elasticsearch.\r\n\r\nThere is another way to interact with the Elasticsearch cluster and that's through the web-based site plugins. You'll find them by selecting Plugins from the side bar. There's Kibana, ElasticHQ, Bigdesk, Head, Paramedic and Kopf, which all have different capabilities, from just monitoring cluster health to helping with query creation and execution.\r\n\r\nYou're next steps with Elasticsearch should be to...\r\n\r\n* Check out the Elasticsearch: The Definitive Guide for how to master Elasticsearch's search capabilities\r\n\r\n* Find out more about the Compose Transporter which can help you get your data from other databases into your Elasticsearch database.","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:30:20 +00:00"}},{"id":"72066661343588428fd0378ef54999edb282a2bd","order":[20150803.0,33],"fields":{},"doc":{"_id":"72066661343588428fd0378ef54999edb282a2bd","_rev":"3-fa4f805dcc1e8a3f8b4c21ca7ea2be7a","created_at":"2015-08-03 14:46:42 +00:00","full_name":"Getting Started with Compose and Bluemix","url":"https://developer.ibm.com/bluemix/2015/07/29/getting-started-compose-bluemix/","status":"Live","name":"Getting Started with Compose and Bluemix","description":"This article walks you through a simple example of how to use Compose’s DBaaS offerings within IBM Bluemix, IBM’s developer cloud platform. ","languages":["Python"],"technologies":["Bluemix","Redis"],"topic":[],"type":"Article","level":"Beginner","imageurl":"","body":"","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:24:28 +00:00"}},{"id":"80dfbfa3fb52ed4fbc5cfbea934f9f2588c75557","order":[20150803.0,36],"fields":{},"doc":{"_id":"80dfbfa3fb52ed4fbc5cfbea934f9f2588c75557","_rev":"5-e469234c04926885b406cb2adb9f3b1d","created_at":"2015-08-03 14:38:47 +00:00","full_name":"Analyzing Salesforce Data with Looker Unblock your CRM data, and make data-driven decisions with your whole team!","url":"https://developer.ibm.com/clouddataservices/simple-data-pipe/","status":"Live","name":"Analyzing Salesforce Data with Looker : A Salesforce BI solution with dashDB, Cloudant, DataWorks, and Looker","description":"The Simple Data Pipe is an app that moves your Salesforce data to dashDB, which is the IBM cloud data warehouse. Once you have your Salesforce data in dashDB, you can do all kinds of analysis on it, with all kinds of tools, such as SQL, R, and Looker.","languages":["Node.js","AngularJS"],"technologies":["dashDB","Looker","Cloudant","Salesforce","DataWorks","OAuth","WebSockets"],"topic":["Data Warehousing","Analytics","Migration"],"type":"Tutorial","level":"Intermediate","imageurl":"http://developer.ibm.com/clouddataservices/wp-content/uploads/sites/47/2015/07/simple-data-pipe.png","body":"","githuburl":"https://github.com/ibm-cds-labs/pipes","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-10 19:03:57 +00:00"}},{"id":"47ed002fdf5e12f0ec2ac4325ae1300b22583e22","order":[20150731.0,26],"fields":{},"doc":{"_id":"47ed002fdf5e12f0ec2ac4325ae1300b22583e22","_rev":"3-0b2337e925a2ec55d691bf5384c61a2a","created_at":"2015-07-31 16:28:16 +00:00","full_name":"Elasticsearch Tools & Compose","url":"https://www.compose.io/articles/elasticsearch-tools-and-compose/","status":"Live","name":"Elasticsearch Tools & Compose","description":"There's a world of tools that make the Elasticsearch even more useful and accessible. In this article we'll look at some and show what you do to get them working with Compose's Elasticsearch deployments. ","languages":["HTTP","Java","JavaScript"],"technologies":["Elasticsearch"],"topic":[],"type":"Article","level":"Beginner","imageurl":"","body":"Outside of the core Elasticsearch toolset, there's a world of tools that make the search and analytics database even more useful and accessible. In this article we'll look at some and show what you do to get them working with Compose's Elasticsearch deployments. We'll start with a command line tool, move on to a simple search tool and finish with an all purpose client for searching and manipulating your Elasticsearch database...\r\n\r\nLet us start the tool tour with Es2unix, from the Elasticsearch developers. Es2unix is a version of the Elasticsearch API that you can use from the command line. It doesn't just make the API calls though, it also converts the returned results into a line-oriented, tabular format like many other Unix tools output. That makes it ideal for integrating Elasticsearch into your awk, grep and sort using shell scripts.\r\n\r\nEs2unix will need Java installed, Java 7 at least, and the binary version can be simply downloaded with a curl command and enabled with chmod as per the installation instructions:\r\n\r\ncurl -s download.elasticsearch.org/es2unix/es >~/bin/es\r\n\r\nchmod +x ~/bin/es\r\n\r\nNote this assumes you have a bin directory in your $HOME and it's on your path.\r\n\r\nNow, when you run es it'll assume that Elasticsearch is running locally. When you are using Compose Elasticsearch, that isn't the case. If you've got the HTTP/TCP access portal enabled, you'll have to give the es command a URL to locate your Elasticsearch deployment. You can get the URL from your Compose dashboard - remember to substitute in the username and password of a Elasticsearch user (from the Users tab) into the URL. This URL is then passed using the -u option:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ version\r\n\r\nes            20140723711d4f9\r\n\r\nelasticsearch 1.3.4\r\n\r\nThe es command is followed by one of a selection of subcommands. There we've used the version subcommand to get the version of the es command and the version of Elasticsearch it is talking to. The health of the cluster can be established with the health subcommand:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ health -v\r\n\r\ntime     cluster    status nodes data pri shards relo init unassign\r\n\r\n11:14:39 EsExemplum green      3    3   3      6    0    0        0\r\n\r\nDrop the -v to get unlabelled results, ideal for passing into monitoring software - adding -v on many es subcommands is a signal that more extensive labelling of returned data is desired.\r\n\r\nThe es command has the ability to count all documents or the number of documents that meets a simple query, and to search all indices and return matching ids:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ count \"one species or variety\"\r\n\r\n11:44:02 16 \"one species or variety\"\r\n\r\nshows a count of documents matching the parts of that phrase to different extents. Using the search command we can dig deeper:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ -v search \"one species or variety\"\r\n\r\nscore   index         type    id\r\n\r\n0.16337 darwin-origin chapter II\r\n\r\n0.12559 darwin-origin chapter IX\r\n\r\n0.10360 darwin-origin chapter IV\r\n\r\n0.10141 darwin-origin chapter I\r\n\r\n0.09734 darwin-origin chapter XI\r\n\r\n0.09326 darwin-origin chapter V\r\n\r\n0.09226 darwin-origin chapter XV\r\n\r\n0.08744 darwin-origin chapter XIV\r\n\r\n0.08069 darwin-origin chapter VIII\r\n\r\n0.07525 darwin-origin chapter III\r\n\r\nTotal: 16\r\n\r\nNow we can see the matching score along with the id, index and type of the document. Although here, 16 documents match, Elasticsearch returns only the top ten results by default. If we wanted to be more precise  we could quote the string (remembering we're in the shell so back-slash escapes are needed) and select a field for matching:\r\n\r\n$ es -u https://user:pass@haproxy1.dblayer.com:10360/ -v search \"\"one species or variety\"\" text\r\n\r\nscore   index         type    id text\r\n\r\n0.03073 darwin-origin chapter I  [\"CHAPTER I. VARIATI\r\n\r\n0.03073 darwin-origin chapter IX [\"CHAPTER IX. HYBRID\r\n\r\nTotal: 2\r\n\r\nOther subcommands in es2unix include indices, for listing indexes, ids for retrieving all ids from an index and a variety of management reporting commands such as nodes, heap and shards.\r\n\r\nYou'll have probably noticed that the es command is a little laborious when you have to specify the URL every time. Es2unix doesn't have any short cuts when it comes to passing that URL like environment variables. There is another way though to shorten things and thats by using an SSH access portal instead. If you configure an SSH access portal for your Elasticsearch deployment then the default command for creating your SSH tunnels makes a node of the cluster appear to be at localhost:9200 which is the default. Once you have an SSH tunnel set up, you can drop the entire -u [URL] part and use tools as if you had Elasticsearch locally configured.\r\n\r\nSometime you just want to set up a quick search for your Elasticsearch database with the minimum of effort. The Calaca project is very useful in that regard. It's an all JavaScript search front end for Elasticsearch which connects up to Elasticsearch. To get up and running, you'll want to download and unpack the zip file available from the Github page. Calaca's configuration can be found in the file js/config.js which looks like this:\r\n\r\nvar indexName = \"name\"; //Ex: twitter\r\n\r\nvar docType = \"type\"; //Ex: tweet\r\n\r\nvar maxResultsSize = 10;\r\n\r\nvar host = \"localhost\"; //Ex: http://ec2-123-aws.com\r\n\r\nvar port = 9200;\r\n\r\nAs you can see, it comes configured to use the database on localhost port 9200, so you could use the SSH shortcut above. But we're here anyway so we need to change the host variable to \"https://user:pass@haproxy1.dblayer.com\" to match the URL we're given in the Compose dashboard and don't forget to copy in the username and password. The port number also needs to be copied from the dashboard URL to the port variable. The rest of the configuration is selecting what to search and what to show. Set the indexName and docType variables to index and data type you want to search. So, for our example here we have a config.js that reads:\r\n\r\nvar indexName = \"darwin-origin\";\r\n\r\nvar docType = \"chapter\";\r\n\r\nvar maxResultsSize = 10;\r\n\r\nvar host = \"https://user:pass@haproxy1.dblayer.com\";\r\n\r\nvar port = 10361;\r\n\r\nThen it's a matter of editing the index.html file to set what results are shown. In the middle of the file is a section which says:\r\n\r\nEdit the result.name and result.description to display what fields you want to display from your document:\r\n\r\nWe have a particularly long block of text in our document which we truncates down and we use the id and title together to create a heading. Save that, open index.html in your browser – there's no need to deploy to a server – and you'll see Calaca's search field. Enter a term and you'll see results like so:\r\n\r\nIt's a quick way to get a pretty search query front end up locally without wrestling with forming Curl/JSON requests or deploying a full on server.\r\n\r\nWhere Calaca's great for a super simple search client, you might want something a little more potent for your searching. For that, try ESClient, which not only has an extensive search UI but adds the ability to display those results in a table or as raw JSON results and then edit and delete selected documents. Like Calaca, ESClient needs no server, just download the zip or clone the Github respository.  Configuring it means just editing the config.js file and putting in the URL from the Compose dashboard:\r\n\r\nvar Config = {\r\n\r\n'CLUSTER_URL':'https://user:pass@haproxy1.dblayer.com:10361',\r\n\r\nThen you open esQueryClient.html in your browser and before you know it, there's the ESClient configuration screen - click the Connect button and a connection to the Elasticsearch database will be made and you'll be moved to the Search tab where you can select index, type, fields, sort fields, specify a Lucene or DSL query and click Search to see the results in a table below the query.\r\n\r\nDouble clicking on a result will let you edit the documents that make up the result or you can use the results as a guide for a delete operation. If you set to \"Raw JSON\" switch in the Configuration tab, you'll also be able to view the complete raw returned results in the JSON Results tab.\r\n\r\nIt's all rather usefully functional and there's only one slight problem. If you look at the top of the ESClient page, you'll see it's displaying the username and password as part of the URL for the database you are connecting to. Not really ideal that, but the SSH access portal can help out there too. If you set up and activate the tunnel, then you can return the CLUSTER_URL value in the config.js file to http://localhost:9200 and there'll be no username or password to display on screen.\r\n\r\nWe've touched on three tools in this article, but more importantly we've shown the practical differences between using the HTTP/TCP and SSH access portals on componse. With HTTP/TCP access, there will be usernames and passwords embedded in the URL you use and this will leave any scripts or tools you configure susceptible to shoulder surfers and the like. That said, for occasionally launched tools it is quick and simple.\r\n\r\nWith the SSH access portal, the configuration and authentication is done when you set up the tunnel in a separate process and the tunnel means you can use Elasticsearch as if the node was installed locally. The downside is you do need to make sure the SSH tunnel is up before you run any command and it may be easier to go through the HTTP/TCP access portal. But then thats why we give you both options at Compose so you can choose what suits you and your applications best.","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:24:01 +00:00"}},{"id":"6449d51ed4936ed705365837e21bcabbe52158f0","order":[20150731.0,27],"fields":{},"doc":{"_id":"6449d51ed4936ed705365837e21bcabbe52158f0","_rev":"5-84bf980ca9057aa19299b1170265ba27","created_at":"2015-07-31 15:46:19 +00:00","full_name":"MongoDB 2.6 Shell & Tools","url":"https://www.compose.io/articles/mongodb-2-6-shell-and-tools-things-worth-knowing/","status":"Live","name":"MongoDB 2.6 Shell & Tools","description":"Customize your Mongo shell experience","languages":["JavaScript"],"technologies":["MongoDB"],"topic":[],"type":"Article","level":"Intermediate","imageurl":"","body":"Earlier this year, as part of a series on the Mongo Shell, we introduced readers to the .mongorc.js file. This file, which lives in a user's home directory, is automatically executed when the mongo shell is run, making it a great place to customise your Mongo shell experience. You can, for example, load it with time-saving functions and pre-initialised variables. In MongoDB 2.6 there's now another startup file, /etc/mongorc.js, which is a global shell startup file that is run before the user's own .mongorc.js file. If a number of users share various tools in the shell, it's really useful to have them in /etc/mongorc.js, as long as everyone has read permission for that file of course.\r\n\r\nOne thing you can't do is stop the global file from being run; the user's own startup file can be disabled with --norc as an option to mongo, but that doesn't affect the global file. There's two things this means. First, the global file is also a good place to put commands you want people to use and not have the excuse they didn't realise they weren't loading them. Secondly, you need to make really sure that your /etc/mongorc.js is valid and correct.\r\n\r\nOutside the shell one of the smaller changes could save you a few keystrokes or cost you a few key if you don't know about it. The mongoimport command takes as an option --collection (also -c), which lets you specify which collection the imported data is going into – in 2.4 and before it was pretty much mandatory. In MongoDB 2.6, you can drop the --collection option and the importer will use the name of the file you are importing from as the name of the collection. So, you can save keystrokes by exporting your data to a file with the collection name its destined for (or be caught out when you accidentally forget the -c option and wonder where that new collection in your database came from).\r\n\r\nIf you've ever felt that the MongoDB executables were somewhat noisy, the --quiet option may bring some calm to your day. In 2.6 the  --quiet option, which suppresses all logging messages except errors, is supported by all the MongoDB command-line tools rather than a select few.","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:24:15 +00:00"}},{"id":"2833b5a6ea9a6119d08ef462c5f9ae92efe1f8be","order":[20150731.0,27],"fields":{},"doc":{"_id":"2833b5a6ea9a6119d08ef462c5f9ae92efe1f8be","_rev":"3-1bcbfeca6bb499efa7400f61d3b97885","created_at":"2015-07-31 16:06:16 +00:00","full_name":"Is PostgreSQL Your Next JSON Database?","url":"https://www.compose.io/articles/is-postgresql-your-next-json-database/","status":"Live","name":"Is PostgreSQL Your Next JSON Database?","description":"With the most recent version of PostgreSQL gaining ever more JSON capabilities, we've been asked if PostgreSQL could replace MongoDB as a JSON database.","languages":["SQL"],"technologies":["MongoDB","PostgreSQL"],"topic":[],"type":"Article","level":"Intermediate","imageurl":"","body":"TL;DR: Betteridge's law applies unless your JSON is fairly unchanging and needs to be queried a lot.\r\n\r\nWith the most recent version of PostgreSQL gaining ever more JSON capabilities, we've been asked if PostgreSQL could replace MongoDB as a JSON database. There's a short answer to that, but we'd prefer to show you. Ah, a question from the audience...\r\n\r\nYes, it did. Before PostgreSQL 9.4 there was the JSON data type and that's still available. It lets you do this:\r\n\r\n>CREATE TABLE justjson ( id INTEGER, doc JSON)\r\n\r\n>INSERT INTO justjson VALUES ( 1, '{\r\n\r\n\"name\":\"fred\",\r\n\r\n\"address\":{\r\n\r\n\"line1\":\"52 The Elms\",\r\n\r\n\"line2\":\"Elmstreet\",\r\n\r\n\"postcode\":\"ES1 1ES\"\r\n\r\nThat stored the raw text of the JSON data in the database, complete with white space and retaining all the orders of any keys and any duplicate keys. Let's show that by looking at the data:\r\n\r\n>SELECT * FROM justjson;\r\n\r\nid |               doc\r\n\r\n1 | {                              +\r\n\r\n|     \"name\":\"fred\",             +\r\n\r\n|     \"address\":{                +\r\n\r\n|         \"line1\":\"52 The Elms\", +\r\n\r\n|         \"line2\":\"Elmstreet\",   +\r\n\r\n|         \"postcode\":\"ES1 1ES\"   +\r\n\r\n(1 row)\r\n\r\nIt has stored an exact copy of the source data. But we can still extract data from it. To do that, there's a set of JSON operators to let us refer to elements within the JSON document. So say we just want the address section, we can do:\r\n\r\nselect doc->>'address' FROM justjson;\r\n\r\n?column?\r\n\r\n\"line1\":\"52 The Elms\", +\r\n\r\n\"line2\":\"Elmstreet\",   +\r\n\r\n\"postcode\":\"ES1 1ES\"   +\r\n\r\n(1 row)\r\n\r\nThe ->> operator says within doc, look up the JSON object with the following fieldname and return it as text. With a number, it would have treated it as an array index, but still returned the value as text. There's also -> to go with ->> which doesn't do that conversion to text. We need that so we can navigate into the JSON objects like so:\r\n\r\nselect doc->'address'->>'postcode' FROM justjson;\r\n\r\n?column?\r\n\r\nES1 1ES\r\n\r\n(1 row)\r\n\r\nThough there is a shorter form where we can specify a path to the data we are after using #>> and an array like this:\r\n\r\nselect doc#>>'{address,postcode}' FROM justjson;\r\n\r\n?column?\r\n\r\nES1 1ES\r\n\r\n(1 row)\r\n\r\nBy preserving the entire document the JSON data type made it easy to work with exact copies of JSON documents and pass them on without loss.  But with that exactness comes a cost, a loss of efficency, and with that comes an inability to index.. So although it's convenient to preserve and parse JSON documents, there was still plenty of room for improvement and thats where JSONB comes in.\r\n\r\nWell, with JSONB it turns the JSON document into a hierarchy of key/value data pairs. All the white space is discarded, only the last value in a set of duplicate keys is used and the order of keys is lost to the structure dictated by the hashes in which they are stored. If we make a JSONB version of the table we just created, insert some data and look at it:\r\n\r\n>CREATE TABLE justjsonb ( id INTEGER, doc JSONB)\r\n\r\n>INSERT INTO justjsonb VALUES ( 1, '{\r\n\r\n\"name\":\"fred\",\r\n\r\n\"address\":{\r\n\r\n\"line1\":\"52 The Elms\",\r\n\r\n\"line2\":\"Elmstreet\",\r\n\r\n\"postcode\":\"ES1 1ES\"\r\n\r\n>SELECT * FROM justjsonb;\r\n\r\nid |                                                doc\r\n\r\n1 | {\"name\": \"fred\", \"address\": {\"line1\": \"52 The Elms\", \"line2\": \"Elmstreet\", \"postcode\": \"ES1 1ES\"}}\r\n\r\n(1 row)\r\n\r\nWe can see that all the textyness of the data has gone away, replaced with the bare minimum required to represent the data held within the JSON document. This stripping down of data means the JSONB representation moves the parsing work to when the data is inserted, but relieves any later access to the data of the task of parsing it.\r\n\r\nLooked at as key/value pairs, then the JSONB datatype does look a bit like the PostgreSQL HSTORE extension. That's a data type for storing key/value pairs but it is an extension, where JSONB (and JSON) are in the core and HSTORE is one-deep in terms of data structure where JSON documents can have a nested elements. Also, HSTORE stores only strings while JSONB understands strings and the full range of JSON numbers.\r\n\r\nIndexing, indexing everywhere. You can't actually index a JSON datatype in PostgreSQL. You can make an index for it using expression indexes, but that'll cover you for whatever you can put in an expression. So if we wanted to we could do\r\n\r\ncreate index justjson_postcode on justjson ((doc->'address'->>'postcode'));\r\n\r\nAnd the postcode, and nothing else would be indexed.\r\n\r\nWith JSONB, there's support for GIN indexes; a Generalized Inverted Index.  That gives you another set of query operators to work with. These are @> contains JSON,  contained, ? test for string existing, ?| any strings existing and ?& all strings existing.\r\n\r\nThere are two kinds of indexes you can create with the default one, called json_ops, which supports all these operators and an index using jsonb_path_ops which only supports @>. The default index creates an index item for every key and value in the JSON, while the jsonb_path_ops only creates a hash of the keys leading up to a value and the value itself and that's a lot more compact and faster to process than the more complex default. But the default does offer more operations at the cost of consuming more space.  After adding some data to our table, we can do a select looking for a particular post code. If we create the default GIN JSON index and do a query:\r\n\r\nexplain select * from justjsonb where doc @> '{ \"address\": { \"postcode\":\"HA36CC\" } }';\r\n\r\nQUERY PLAN\r\n\r\nSeq Scan on justjsonb  (cost=0.00..3171.14 rows=100 width=123)\r\n\r\nFilter: (doc @> '{\"address\": {\"postcode\": \"HA36CC\"}}'::jsonb)\r\n\r\n(2 rows)\r\n\r\nWe can see that it will sequentially scan the table. Now, if we create a default JSON GIN index we can see the difference it makes:\r\n\r\n> create index justjsonb_gin on justjsonb using gin (doc);\r\n\r\n> explain select * from justjsonb where doc @> '{ \"address\": { \"postcode\":\"HA36CC\" } }';\r\n\r\nQUERY PLAN\r\n\r\nBitmap Heap Scan on justjsonb  (cost=40.78..367.62 rows=100 width=123)\r\n\r\nRecheck Cond: (doc @> '{\"address\": {\"postcode\": \"HA36CC\"}}'::jsonb)\r\n\r\n->  Bitmap Index Scan on justjsonb_gin  (cost=0.00..40.75 rows=100 width=0)\r\n\r\nIndex Cond: (doc @> '{\"address\": {\"postcode\": \"HA36CC\"}}'::jsonb)\r\n\r\n(4 rows)\r\n\r\nIt's a lot more efficient searching as you can tell by the lower cost. But the hidden cost is in the size of the index. In this case it's 41% of the size of the data. Let's drop that index and repeat the process with a jsonbpathops GIN index.\r\n\r\n> create index justjsonb_gin on justjsonb using gin (doc jsonb_path_ops);\r\n\r\n> explain select * from justjsonb where doc @> '{ \"address\": { \"postcode\":\"HA36CC\" } }';\r\n\r\nQUERY PLAN\r\n\r\nBitmap Heap Scan on justjsonb  (cost=16.78..343.62 rows=100 width=123)\r\n\r\nRecheck Cond: (doc @> '{\"address\": {\"postcode\": \"HA36CC\"}}'::jsonb)\r\n\r\n->  Bitmap Index Scan on justjsonb_gin  (cost=0.00..16.75 rows=100 width=0)\r\n\r\nIndex Cond: (doc @> '{\"address\": {\"postcode\": \"HA36CC\"}}'::jsonb)\r\n\r\n(4 rows)\r\n\r\nThe total cost is slightly lower and typically the index size should be a lot smaller. It's going to be the classic task of balancing speed and size for indexes. But it's far more efficient than sequentially scanning.\r\n\r\nIf you update your JSON documents in place, the answer is no. What PostgreSQL is very good at is storing and retrieving JSON documents and their fields. But even though you can individually address the various fields within the JSON document, you can't update a single field. Well, actually you can, but by extracting the entire JSON document out, appending the new values and writing it back, letting the JSON parser sort out the duplicates. But it's likely that you aren't going to want to rely on that.\r\n\r\nIf your active data sits in the relational schema comfortably and the JSON content is a cohort to that data then you should be fine with PostgreSQL and it's much more efficient JSONB representation and indexing capabilities. If though, your data model is that of a collection of mutable documents then you probably want to look at a database engineered primarily around JSON documents like MongoDB or RethinkDB.","githuburl":"","videourl":"","demourl":"","documentationurl":"","otherurl":"","related":[],"featured":false,"updated_at":"2015-08-04 15:23:44 +00:00"}}],"counts":{"language":{"Android":1.0,"AngularJS":1.0,"C#":1.0,"CSS":2.0,"Go":1.0,"HTML5":5.0,"HTTP":13.0,"Java":7.0,"JavaScript":30.0,"Node.js":8.0,"Objective-C":3.0,"PHP":2.0,"Python":6.0,"R":11.0,"Ruby":3.0,"SQL":12.0,"Swift":1.0,"iOS":4.0},"level":{"Advanced":4.0,"Beginner":105.0,"Intermediate":50.0},"technology":{"Bluemix":23.0,"Cloudant":99.0,"Cloudant Local":1.0,"Cordova":1.0,"CouchDB":4.0,"DataWorks":4.0,"Elasticsearch":3.0,"Graph Data Store":1.0,"Ionic":1.0,"Looker":1.0,"MobileFirst":7.0,"MongoDB":8.0,"OAuth":1.0,"PhoneGap":1.0,"PostgreSQL":7.0,"PouchDB":5.0,"Redis":4.0,"Salesforce":1.0,"Spark":1.0,"WebSockets":1.0,"dashDB":43.0},"topic":{"Analytics":31.0,"Data Warehousing":36.0,"Gaming":1.0,"Hybrid":5.0,"IoT":6.0,"Location":15.0,"Migration":15.0,"Mobile":18.0,"NoSQL":83.0,"Offline":11.0,"Open Data":2.0,"SQL":6.0,"Standards":2.0},"type":{"Article":72.0,"Tutorial":32.0,"Video":55.0}}}
